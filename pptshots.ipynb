{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "featured-daily",
   "metadata": {},
   "source": [
    "## PPTSHOTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-craps",
   "metadata": {},
   "source": [
    "Finding sensitive information in the trimmed parts of cropped images \n",
    "\n",
    "\n",
    "Cropping pictures inserted in a powerpoint presentation enables users to hide parts of a picture that they do not want to display. The problem is that Officeâ€™s cropping tool only modifies how the cropped image is displayed in the body of the document. The original picture remains intact. Cropped portions of the image are not completely removed from the document and can be seen by others if the file is uploaded to the internet. Data leakage can occur if there is sensitive data in the trimmed areas.\n",
    "\n",
    "PPTSHOTS searches google for presentations by query, downloads them and checks for images where cropping has occured.\n",
    "\n",
    "This solution uses goog.io, They have free and commercial packages available. Enter your key in the .env file\n",
    "\n",
    "It is advised that you run the notebook in a sandbox or vm as it does involve downloading untrusted documents from the internet.\n",
    "\n",
    "\n",
    "Clone the repository\n",
    "\n",
    "`git clone https://github.com/dfaram7/pptshots.git`\n",
    "\n",
    "\n",
    "Install the requirements\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "Run the notebook!\n",
    "\n",
    "`jupyter notebook`\n",
    "\n",
    "If you dont want to read all the code you can just SHIFT+ENTER down to th eexclamation marks and enter your search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-asbestos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "composite-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "import requests\n",
    "import urllib.request\n",
    "import xml.dom.minidom                                        \n",
    "from time import sleep\n",
    "import random\n",
    "import tldextract\n",
    "import urllib\n",
    "import wget\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "import zipfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liked-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() #loads variables from .env\n",
    "key = os.getenv('GOOG') #loads api key - get a free account at goog.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "increasing-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the api key in headers\n",
    "headers = {\n",
    "    \"apikey\": key\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "atlantic-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion that processes a pptx file and saves the images in img_folder\n",
    "def process(pptx, img_dir=None):\n",
    "    \n",
    "    # unzip the docx in memory\n",
    "    zipf = zipfile.ZipFile(re.sub(r'[\\\\/*?:\"<>|]',\"\", pptx))\n",
    "    filelist = zipf.namelist()\n",
    "\n",
    "    if img_dir is not None:\n",
    "        # extract images\n",
    "        for fname in filelist:\n",
    "            _, extension = os.path.splitext(fname)\n",
    "            if extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
    "                dst_fname = os.path.join(img_dir, os.path.basename(fname))\n",
    "                with open(dst_fname, \"wb\") as dst_f:\n",
    "                    dst_f.write(zipf.read(fname))\n",
    "\n",
    "    zipf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "comfortable-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that evaluates a pptx file for any significant area that are hidden in the \"trimmmed\" section of a cropped image\n",
    "def evaluator(ppt):\n",
    "    prs = Presentation(ppt)\n",
    "    percentages = []\n",
    "    for x in prs.slides:\n",
    "\n",
    "        for y in x.element.xpath('//a:srcRect'):\n",
    "            l = y.l #distance from left edge as %\n",
    "            b = y.b #distance from bottom edge as %\n",
    "            r = y.r #distance from right edge as %\n",
    "            t= y.t #distance from top edge as %\n",
    "            if ((l+r)*(t+b)) != 0.0:\n",
    "                percentages.append((l+r)*(t+b))\n",
    "    return percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "solar-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses goog.io api to search google and returns list of urls\n",
    "def googer(term, typ):\n",
    "    \n",
    "    query = {\n",
    "    \"q\": term,\n",
    "    \"hl\": \"en\",\n",
    "    \"num\": 70\n",
    "    }\n",
    "    url = f\"https://api.goog.io/v1/search/\" + urllib.parse.urlencode(query)\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    results = resp.json()\n",
    "    temp_urls = []\n",
    "    for x in results['results']:\n",
    "        temp_urls.append([x['link'], typ])\n",
    "    #returns list of lists containing [url, file extension]\n",
    "    return temp_urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "polished-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles appending search term with 'filetype:pptx'\n",
    "def get_urls(searchterm):\n",
    "    final_list = []\n",
    "    url_list = []\n",
    "    url_list.append(googer(searchterm + \" filetype:pptx\", 'pptx'))\n",
    "    for url in [item for sublist in url_list for item in sublist]:\n",
    "        try:   \n",
    "            r = requests.get(url[0])\n",
    "            sc = r.status_code\n",
    "            if sc == 200:\n",
    "                final_list.append(url)\n",
    "        except:\n",
    "            pass\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "continent-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where documents are autodownloded, a file extension is added\n",
    "def filenamer(url_link, doc_name):\n",
    "    name = ''\n",
    "    if url_link.endswith('pptx'):\n",
    "        name = doc_name\n",
    "    else:\n",
    "        name = re.sub(r'[\\\\/*?:\"<>|]',\"\", doc_name) + '.pptx'\n",
    "    return name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fuzzy-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloads pptx file to working directory\n",
    "def save_link(url_link, doc_name):\n",
    "    if url_link.endswith('pptx'):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url_link, doc_name)\n",
    "        except:\n",
    "            test = requests.get(url_link)\n",
    "            with open(re.sub(r'[\\\\/*?:\"<>|]',\"\", doc_name), 'wb') as f:\n",
    "                f.write(test.content)\n",
    "    else:\n",
    "        test = requests.get(url_link)\n",
    "        with open(re.sub(r'[\\\\/*?:\"<>|]',\"\", doc_name) +'.pptx', 'wb') as f:\n",
    "            f.write(test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "advisory-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where documents are autodownloded, a file extension is checked\n",
    "def extension_check(doc, typ):\n",
    "    name = ''\n",
    "    if str(doc).endswith('pptx'):\n",
    "        name = doc\n",
    "    else:\n",
    "        name = re.sub(r'[\\\\/*?:\"<>|]',\"\", doc) + '.' + typ\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "graduate-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except:\n",
    "        return 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "guided-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each url, the document will have the images extracted and the size of the trimmed area measured\n",
    "def prioritiser(urls):\n",
    "  \n",
    "    priorities = []\n",
    "    \n",
    "    for u in urls:\n",
    "        #print(u[0])\n",
    "        url = u[0] #url\n",
    "        typ = u[1] #file extension\n",
    "        \n",
    "        if os.path.isdir('img_folder/' + tldextract.extract(u[0]).domain) is False:\n",
    "            os.mkdir('img_folder/' + tldextract.extract(u[0]).domain)  \n",
    "        try:\n",
    "            save_link(url, url.split('/')[-1]) #saves pptx to folder\n",
    "\n",
    "            #saves images into domain labeled folder\n",
    "            process(filenamer(url, url.split('/')[-1]), 'img_folder/' + tldextract.extract(u[0]).domain + '/')\n",
    "            pic_list = evaluator(filenamer(url, url.split('/')[-1]))\n",
    "            \n",
    "            if average(pic_list) != 0.0:\n",
    "                priorities.append([tldextract.extract(u[0]).domain, url, pic_list, average(pic_list)])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    return priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "municipal-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to allow for a single search term to be enetered\n",
    "def pptshots(term):\n",
    "    return prioritiser(get_urls(term))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-wonder",
   "metadata": {},
   "source": [
    "# !!!!!!! Enter search term below - maybe check out your own domain with the site:domain.com dork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-killing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package not found at 'filedownload.ashx?moduleinstanceid=25563&dataid=41911&FileName=Investigate_%20How%20Do%20I%20Search%20for%20Relevant%20Resources_.pptx'\n",
      "file 'a-primer-on-searching-the-internet-v2021.ppsx.pptx' is not a PowerPoint file, content type is 'application/vnd.openxmlformats-officedocument.presentationml.slideshow.main+xml'\n"
     ]
    }
   ],
   "source": [
    "#this can take a while for all the urls to be processed so be patient\n",
    "#prints a list of lists with the [[domain, url of the pptx, [list of images in the pptx], average % of image cropped]]\n",
    "#any powerpoint that has a gretaer tham >20% area cropped is maybe worth a look\n",
    "#browse to img_folder/domain and have a look for anything interetsing\n",
    "pptshots('your search term or domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for documents hosed on domains that have a hackerone bug bounty progamme \n",
    "\n",
    "'''\n",
    "def bug_bounty_domains():\n",
    "    jsn = requests.get('https://raw.githubusercontent.com/disclose/diodb/master/program-list.json').json()\n",
    "    domainlist = []\n",
    "    for j in jsn:\n",
    "        parsed = tldextract.extract(j['policy_url'])\n",
    "        domainlist.append('.'.join([parsed.domain, parsed.suffix]))\n",
    "    return domainlist\n",
    "\n",
    "bug_list = bug_bounty_domains()\n",
    "random.shuffle(bug_list)\n",
    "\n",
    "for bug in bug_list:\n",
    "    try:\n",
    "        for l in pptshots('site:' + bug):\n",
    "            print(l)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.error('HTTP ERROR ' + str(e))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "'''\n",
    "def tld_getter():\n",
    "    tld_list = []\n",
    "    with requests.Session() as s:\n",
    "        download = s.get('https://gist.githubusercontent.com/derlin/421d2bb55018a1538271227ff6b1299d/raw/3a131d47ca322a1d001f1f79333d924672194f36/country-codes-tlds.csv')\n",
    "\n",
    "        decoded_content = download.content.decode('utf-8')\n",
    "\n",
    "        cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "        \n",
    "        for country in list(cr)[1:]:\n",
    "            tld_list.append(country[1].lstrip())\n",
    "    return tld_list\n",
    "\n",
    "ctry_list = tld_getter()\n",
    "random.shuffle(ctry_list)\n",
    "\n",
    "for ctry in ctry_list:\n",
    "    try:\n",
    "        print(ctry + ' ' + str(pptshots('site:' + 'gov' + ctry)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.error('HTTP ERROR ' + str(e))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noted-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#search across fortune 500 domains\n",
    "'''\n",
    "\n",
    "def fortune_getter():\n",
    "    f_list = []\n",
    "    with requests.Session() as s:\n",
    "        download = s.get('https://gist.githubusercontent.com/mbejda/45db05ea50e79bc42016/raw/52d5ca99398b495e096f6eace20f5872129633e3/Fortune-1000-Company-Twitter-Accounts.csv')\n",
    "\n",
    "        decoded_content = download.content.decode('utf-8')\n",
    "\n",
    "        cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "        \n",
    "        for country in list(cr)[1:]:\n",
    "            f_list.append(country[0].lstrip())\n",
    "    return f_list\n",
    "\n",
    "ftn_list = fortune_getter()\n",
    "random.shuffle(ftn_list)\n",
    "\n",
    "for ftn in ftn_list:\n",
    "    try:\n",
    "        print(ftn + ' ' + str(pptshots('site:' + ftn)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.error('HTTP ERROR ' + str(e))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-grocery",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "It is actually pretty rare to find anything interesting, after several days only one presentation contained 'sensitive' information. In this instance an \"unnamed US federal government executive branch organization\" had unintentionally left some PII in a Facebook screenshot. I reported this to them and the presentation is no longer publicly facing.\n",
    "\n",
    "Other less sensitive information included browser tabs and OS information from the screen peripheries which could be of minor value to an attacker but nothing too exciting. Interestingly, on a few occasions where screenshots had been taken with dual monitors there was an entire extra screen to examine - I didn't identify anything more valuable than a half filled in timesheet but there is potential for sizeable data to have been exposed if a spreadsheet or similar had been open.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "finished-crossing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "anonymous-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
